layers:
  - hidden:
     units: 500
     type: dense
     activation: relu
     init_scale: 0.03
  - hidden:
     units: 500
     type: dense
     activation: relu
     init_scale: 0.03
  - output:
     type: dense
     activation: linear
     init_scale: 0.03