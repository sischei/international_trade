layers:
  - hidden:
     units: 1000
     type: dense
     activation: relu
     init_scale: 0.1
     batch_normalize:
       momentum: 0.99
  - hidden:
     units: 1000
     type: dense
     activation: relu
     init_scale: 0.1
     batch_normalize:
       momentum: 0.99
  #- hidden:
     #units: 200
     #type: dense
     #activation: relu
     #init_scale: 0.1
     #batch_normalize:
       #momentum: 0.99
  #- hidden:
     #units: 200
     #type: dense
     #activation: relu
     #init_scale: 0.1
     #batch_normalize:
       #momentum: 0.99       
  #- hidden:
     #units: 200
     #type: dense
     #activation: relu
     #init_scale: 0.1
     #batch_normalize:
       #momentum: 0.99        
  - output:
     type: dense
     activation: linear
     init_scale: 0.1
# --------------------------------------------------------------------------- #
# Mimic the Glorot uniform initializer using the VarianceScaling initializer
# --------------------------------------------------------------------------- #
#net_initializer_mode: fan_avg
#net_initializer_distribution: uniform   
net_initializer_mode: fan_in
net_initializer_distribution: truncated_normal
